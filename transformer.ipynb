{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def _init_(self,\n",
    "               src_pad_idx,\n",
    "               trg_pad_idx,\n",
    "               enc_voc_size,\n",
    "               dec_voc_size,\n",
    "               d_model,\n",
    "               max_len,\n",
    "               n_heads,\n",
    "               ffn_hidden,\n",
    "               n_layers,\n",
    "               drop_prob,\n",
    "               device):\n",
    "        super(Transformer, self)._init_()\n",
    "        self.encoder = Encoder(\n",
    "            enc_voc_size, \n",
    "            max_len, \n",
    "            d_model, \n",
    "            ffn_hidden, \n",
    "            n_heads, \n",
    "            n_layers, \n",
    "            drop_prob,\n",
    "            device,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            dec_voc_size,\n",
    "            max_len, \n",
    "            d_model, \n",
    "            ffn_hidden, \n",
    "            n_heads, \n",
    "            n_layers, \n",
    "            drop_prob, \n",
    "            device\n",
    "        )\n",
    "        self.src_pad.idx = src_pad_idx\n",
    "        self.trg_pad.idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    def make_pad_mask(self, q, k, pad_idx_q, pad_idx_k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        q = q.ne(pad_idx_q).uniqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "        k = k.ne(pad_idx_k).uniqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "\n",
    "        mask = q & k\n",
    "        return mask\n",
    "    \n",
    "    def make_casual_mask(self, q, k):\n",
    "        mask = torch.trill(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * self.make_casual_mask(trg, trg)\n",
    "        enc = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, src, trg_mask, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
